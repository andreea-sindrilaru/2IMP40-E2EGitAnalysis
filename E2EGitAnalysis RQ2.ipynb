{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e153c8",
   "metadata": {},
   "source": [
    "RQ2: To what extent do non-trivial open-source repositories that implement both GUI and performance end-to-end tests differ from those that implement only GUI end-to-end tests or only performance end-to-end tests, with respect to project activity metrics such as the number of commits, contributors, issues, and pull requests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239ca76",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d542f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030ef41",
   "metadata": {},
   "source": [
    "### Load CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982182c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repository_general = pd.read_csv('E2EGit\\\\repository.csv')\n",
    "df_repository_general = df_repository_general.rename(columns={'name': 'repository_name'})\n",
    "df_repository_general = df_repository_general[['repository_name', 'commits','contributors', 'total_issues', 'total_pull_requests']]\n",
    "\n",
    "df_filtered = df_repository_general[\n",
    "    (df_repository_general['commits'] >= 2000) &\n",
    "    (df_repository_general['contributors'] >= 10) &\n",
    "    (df_repository_general['total_issues'] >= 100) &\n",
    "    (df_repository_general['total_pull_requests'] >= 50)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "df_filtered.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c2215",
   "metadata": {},
   "source": [
    "Load non_trivial_repository CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repository_non_trivial = pd.read_csv('E2EGit\\\\non_trivial_repository.csv')\n",
    "\n",
    "df_repository_non_trivial = df_repository_non_trivial.rename(columns={'name': 'repository_name'})\n",
    "\n",
    "df_repository_non_trivial = df_repository_non_trivial[['repository_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb746ca1",
   "metadata": {},
   "source": [
    "Merge general info with non trivial repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repository = pd.merge(df_filtered, df_repository_non_trivial, left_on='repository_name', right_on='repository_name', how='inner')\n",
    "\n",
    "print(len(df_repository))\n",
    "\n",
    "print(df_repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8800f",
   "metadata": {},
   "source": [
    "Clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['commits', 'contributors', 'total_issues', 'total_pull_requests']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_repository[col] = pd.to_numeric(df_repository[col], errors='coerce')\n",
    "\n",
    "# drop NaNs\n",
    "df_repository = df_repository.dropna(subset=numeric_cols)\n",
    "\n",
    "print(df_repository)\n",
    "print(len(df_repository))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f311f",
   "metadata": {},
   "source": [
    "Load GUI details CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0560f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gui_repo_details = pd.read_csv('E2EGit\\gui_testing_repo_details.csv')\n",
    "\n",
    "# Keep only the columns you want\n",
    "df_gui_repo_details = df_gui_repo_details[['repository_name']]\n",
    "\n",
    "print(df_gui_repo_details)\n",
    "print(len(df_gui_repo_details))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31740d2",
   "metadata": {},
   "source": [
    "Load performance details CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caecfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance_test_details = pd.read_csv('E2EGit\\performance_testing_test_details.csv')\n",
    "df_performance_test_details = df_performance_test_details[['repository_name']]\n",
    "\n",
    "print(df_performance_test_details)\n",
    "print(len(df_performance_test_details))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ed13b",
   "metadata": {},
   "source": [
    "Merge performance with GUI testing to get repositories that implement both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d158be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_both_tests = pd.merge(df_performance_test_details, df_gui_repo_details, left_on='repository_name', right_on='repository_name', how='inner')\n",
    "\n",
    "print(df_both_tests)\n",
    "print(len(df_both_tests))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9033b",
   "metadata": {},
   "source": [
    "Get reposorties that implement GUI only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f20e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gui_only = pd.merge(df_gui_repo_details, df_performance_test_details, on='repository_name', how='left', indicator=True)\n",
    "df_gui_only = df_gui_only[df_gui_only['_merge'] == 'left_only'][['repository_name']]\n",
    "\n",
    "print(df_gui_only)\n",
    "print(len(df_gui_only))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818eeca0",
   "metadata": {},
   "source": [
    "Get reposorties that implement Performance only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177edf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_only = pd.merge(df_performance_test_details, df_gui_repo_details, on='repository_name', how='left', indicator=True)\n",
    "df_perf_only = df_perf_only[df_perf_only['_merge'] == 'left_only'][['repository_name']]\n",
    "\n",
    "print(df_perf_only)\n",
    "print(len(df_perf_only))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ce018",
   "metadata": {},
   "source": [
    "Merge both Dataframe with the Dataframe that contains project activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_both_with_repository_details = pd.merge(df_both_tests, df_repository, on='repository_name', how='inner')\n",
    "\n",
    "repo_both_with_repository_details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1f87e",
   "metadata": {},
   "source": [
    "Not merged repositories (exist in GUI or Performance but not in both) with repository details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_gui_only_with_repository_details = pd.merge(df_gui_only, df_repository, on='repository_name', how='inner')\n",
    "repo_perf_only_with_repository_details = pd.merge(df_perf_only, df_repository, on='repository_name', how='inner')\n",
    "\n",
    "print(len(repo_gui_only_with_repository_details))\n",
    "print(len(repo_perf_only_with_repository_details))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6cb06",
   "metadata": {},
   "source": [
    "Add a column to each DataFrame to identify their type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_both_with_repository_details['test_type'] = 'Both'\n",
    "repo_gui_only_with_repository_details['test_type'] = 'GUI'\n",
    "repo_perf_only_with_repository_details['test_type'] = 'Performance'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd407b24",
   "metadata": {},
   "source": [
    "Get the final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([repo_both_with_repository_details, repo_gui_only_with_repository_details, repo_perf_only_with_repository_details], ignore_index=True)\n",
    "\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c8142a",
   "metadata": {},
   "source": [
    "###  Normality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1023fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_results = {}\n",
    "\n",
    "for test_type in df_all['test_type'].unique():\n",
    "    print(f\"{test_type}:\")\n",
    "    subset = df_all[df_all['test_type'] == test_type]\n",
    "    normality_results[test_type] = {}\n",
    "    \n",
    "    for metric in numeric_cols:\n",
    "        stat, p_value = stats.shapiro(subset[metric])\n",
    "        is_normal = p_value > 0.05\n",
    "        normality_results[test_type][metric] = is_normal\n",
    "        print(f\"  {metric:15s}: W={stat:.4f}, p={p_value:.4f} | \"\n",
    "              f\"{'Normal' if is_normal else 'Non normal'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = df_all.groupby('test_type').agg({\n",
    "    'commits': ['mean', 'median', 'std', 'count'],\n",
    "    'contributors': ['mean', 'median', 'std'],\n",
    "    'total_issues': ['mean', 'median', 'std'],\n",
    "    'total_pull_requests': ['mean', 'median', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ace657",
   "metadata": {},
   "source": [
    "### Mann-Whitney U test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a042866",
   "metadata": {},
   "outputs": [],
   "source": [
    "gui_only = df_all[df_all['test_type'] == 'GUI']\n",
    "perf_only = df_all[df_all['test_type'] == 'Performance']\n",
    "both_tests = df_all[df_all['test_type'] == 'Both']\n",
    "\n",
    "comparisons = [\n",
    "    ('Both', 'GUI Only', both_tests, gui_only),\n",
    "    ('Both', 'Performance Only', both_tests, perf_only),\n",
    "    ('GUI Only', 'Performance Only', gui_only, perf_only)\n",
    "]\n",
    "\n",
    "mw_results = {}\n",
    "\n",
    "for metric in numeric_cols:\n",
    "    print(f\"{metric.upper()}\")\n",
    "    print(\"-\" * 80)\n",
    "    mw_results[metric] = {}\n",
    "    \n",
    "    for label1, label2, data1, data2 in comparisons:\n",
    "        values1 = data1[metric]\n",
    "        values2 = data2[metric]\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        u_stat, p_value = stats.mannwhitneyu(values1, values2, alternative='two-sided')\n",
    "        \n",
    "        # Calculate effect size (rank-biserial correlation)\n",
    "        n1, n2 = len(values1), len(values2)\n",
    "        rank_biserial = 1 - (2*u_stat) / (n1 * n2)\n",
    "        \n",
    "        # Calculate medians\n",
    "        median1 = values1.median()\n",
    "        median2 = values2.median()\n",
    "        median_diff = median1 - median2\n",
    "        pct_diff = (median_diff / median2) * 100 if median2 != 0 else 0\n",
    "        \n",
    "        sig_marker = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "        \n",
    "        mw_results[metric][f\"{label1}_vs_{label2}\"] = {\n",
    "            'u_stat': u_stat,\n",
    "            'p_value': p_value,\n",
    "            'rank_biserial': rank_biserial,\n",
    "            'median_diff': median_diff,\n",
    "            'pct_diff': pct_diff\n",
    "        }\n",
    "        \n",
    "        print(f\"  {label1} vs {label2}\")\n",
    "        print(f\"    Median 1: {median1:.2f} | Median 2: {median2:.2f}\")\n",
    "        print(f\"    Difference: {median_diff:+.2f} ({pct_diff:+.1f}%)\")\n",
    "        print(f\"    U-statistic: {u_stat:.2f}\")\n",
    "        print(f\"    P-value: {p_value:.4f} {sig_marker}\")\n",
    "        print(f\"    Effect size (r): {rank_biserial:.3f}\")\n",
    "    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
